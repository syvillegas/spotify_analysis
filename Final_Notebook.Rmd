---
title: "Final Project"
author: "Sergio-Yersi Villegas Pelegrín - Lorenzo Leskovar Ustarroz"
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: yes
---
```{r message=FALSE, warning=FALSE, include=FALSE, results='hide'}
rm(list = ls())
PATH= '/Users/loren/Universidades/BSE/Statistical Modelling and Inference/'
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(bayestestR)
library(rstanarm)
library(mombf)
library(mvtnorm)
source(file.path(PATH,"final_project_stats/routines_final_project.R")) #We take functions from this file in order to do the cross-validation
seed=2345
```

# Part 1: Regression

## Import Data For Regression

```{r}
data <- read.csv2(file.path(PATH,'final_project_stats/Data_Spotify.csv'), sep=';',header = TRUE)
str(data)
```

## Pre-processing

```{r}
data <- data[data$msPlayed > 0,] #Because we will take the log, we keep only > 0 values in for the target variable
colnames(data)
```

```{r}
y <- data$msPlayed
y_log <- log(y)
```

```{r}
qplot(y, geom="histogram",bins = 30)
qplot(y_log, geom="histogram",bins = 30)
```

```{r}
X <- subset(data,select=-msPlayed)
```


```{r}
X <- model.matrix(~ X.markets + popularity + danceability + energy + key + loudness + mode + speechiness + acousticness + instrumentalness
             + liveness + valence + tempo + duration_ms + time_signature + artist + release_date + explicit+ release_date:danceability 
             + release_date:energy + release_date:loudness + release_date:speechiness + release_date:acousticness 
             + release_date:instrumentalness + release_date:liveness + release_date:valence + release_date:tempo + release_date:duration_ms
             +  popularity:danceability + popularity:energy + popularity:loudness + popularity:speechiness + popularity:acousticness 
             + popularity:instrumentalness + popularity:liveness + popularity:valence + popularity:tempo + popularity:duration_ms 
             + danceability:energy + danceability:loudness + danceability:speechiness + danceability:acousticness + danceability:instrumentalness 
             + danceability:liveness + danceability:valence + danceability:tempo + danceability:duration_ms + energy:loudness + energy:speechiness 
             + energy:acousticness + energy:instrumentalness + energy:liveness + energy:valence + energy:tempo + energy:duration_ms 
             + loudness:speechiness + loudness:acousticness + loudness:instrumentalness + loudness:liveness + loudness:valence + loudness:tempo 
             + loudness:duration_ms + speechiness:acousticness + speechiness:instrumentalness + speechiness:liveness + speechiness:valence 
             + speechiness:tempo + speechiness:duration_ms + acousticness:instrumentalness + acousticness:liveness + acousticness:valence 
             + acousticness:tempo + acousticness:duration_ms + instrumentalness:liveness + instrumentalness:valence + instrumentalness:tempo 
             + instrumentalness:duration_ms + liveness:valence + liveness:tempo + liveness:duration_ms + valence:tempo + valence:duration_ms 
             + tempo:duration_ms, data=X)
dim(X)
```


```{r}
dummies <- c()
    for (column in colnames(X)){
      dummies <- c(dummies,length(unique(X[,column])) == 2)
    }

categorical_features <- colnames(X)[dummies]
numerical_features <- colnames(X)[!dummies]
```

```{r}
for(i in categorical_features) {
    X[,i] <- as.factor(X[,i])
}
```

## MLE

### Cross-Validated Metrics MLE

```{r warning=FALSE}
cv.mle= kfoldCV.mle(y_log,as.data.frame(X[,-1]),K = 10, seed=1)
r2.mle= cor(y_log,cv.mle$pred)^2
mse.mle = mean((y_log - cv.mle$pred)^2)

n <- length(y_log)
p <- as.integer(mean(cv.mle$num_betas))
bic.mle <- n * log(colSums((y_log-as.matrix(cv.mle$pred))^2)/length(y_log)) + n*(log(2*pi)+1) + log(n)*p 

print(paste('R^2 Score:',round(r2.mle,4)))
print(paste('MSE Score:',round(mse.mle,4)))
print(paste('BIC Score:',round(bic.mle,4)))
print(paste('Number of Betas:',p))
```

## LASSO

### Lasso-CV

#### Fit with glmnet Package

```{r}
fit.lasso= cv.glmnet(x=X[,-1], y=y_log, nfolds=10)
fit.lasso
```

```{r}
print(paste('Number of Lambdas evaluated:',length(fit.lasso$lambda)))
plot(fit.lasso)
```

```{r}
plot(fit.lasso$glmnet.fit, xvar='lambda')
```
```{r}
b.lasso= as.vector(coef(fit.lasso, s='lambda.min'))

print(paste('Lambda:',round(fit.lasso$lambda.min,4)))
print(paste('B_0:',sum(b.lasso!=0)))
```

#### Cross-Validated Metrics Lasso-CV

```{r}
cv.lasso= kfoldCV.lasso(y=y_log,x=X[,-1],K=10,seed=1,criterion="cv")
r2.lassocv= cor(y_log,cv.lasso$pred)^2
mse.lassocv = mean((y_log - cv.lasso$pred)^2)

n <- length(y_log)
p <- as.integer(mean(cv.lasso$num_betas))
bic.lassocv <- n * log(colSums((y_log-as.matrix(cv.lasso$pred))^2)/length(y_log)) + n*(log(2*pi)+1) + log(n)*p 

print(paste('R^2 Score:',round(r2.lassocv,4)))
print(paste('MSE Score:',round(mse.lassocv,4)))
print(paste('BIC Score:',round(bic.lassocv,4)))
```

### Lasso-BIC

#### Fit

```{r}
fit.lassobic <- lasso.bic(y_log, X[,-1],extended = FALSE)
print(paste('Lambda:',fit.lassobic$lambda.opt))
print(paste('B_0:',sum(fit.lassobic$coef!=0)))
```


#### Cross-Validated Metrics Lasso-BIC

```{r}
cv.lassobic= kfoldCV.lasso(y=y_log,x=X[,-1],K=10,seed=1,criterion="bic")
r2.lassobic= cor(y_log,cv.lassobic$pred)^2
mse.lassobic = mean((y_log - cv.lassobic$pred)^2)

n <- length(y_log)
p <- as.integer(mean(cv.lassobic$num_betas))
bic.lassobic <- n * log(colSums((y_log-as.matrix(cv.lassobic$pred))^2)/length(y_log)) + n*(log(2*pi)+1) + log(n)*p 

print(paste('R^2 Score:',round(r2.lassobic,4)))
print(paste('MSE Score:',round(mse.lassobic,4)))
print(paste('BIC Score:',round(bic.lassobic,4)))
```

## Bayesian Model Selection and Averaging

### Fit with mombf package

```{r}
fit.bayesreg <- modelSelection(y=y_log,x=X[,-1], priorCoef=zellnerprior(taustd=1),priorVar=igprior(alpha=.01, lambda=.01),
                               priorDelta=modelbbprior(1,1),family = 'normal')
```

```{r}
head(postProb(fit.bayesreg),10)
```
```{r}
names(fit.bayesreg$postMode[fit.bayesreg$postMode==1])
```

```{r}
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
ci.bayesreg[,1:3]= round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4]= round(ci.bayesreg[,4], 4)  
head(ci.bayesreg[order(abs(ci.bayesreg[,'margpp']),decreasing = T),],10)
```

### Cross-Validated Metrics BMA

```{r}
cv_bms <- kfoldCV.bms(y_log,X[,-1],seed=10,K = 10)
r2.bms <- cor(y_log,cv_bms$pred)^2
mse.bms = mean((y_log - cv_bms$pred)^2)

n <- length(y_log)
p <- as.integer(mean(cv_bms$num_betas))
bic.bms <- n * log(colSums((y_log-as.matrix(cv_bms$pred))^2)/length(y_log)) + n*(log(2*pi)+1) + log(n)*p 

print(paste('R^2 Score:',round(r2.bms,4)))
print(paste('MSE Score:',round(mse.bms,4)))
print(paste('BIC Score:',round(bic.bms,4)))
```

### Comparison of LASSO-BIC and BMA

```{r}
len_bayes <- length(ci.bayesreg[,1][ci.bayesreg[,1] != 0])
len_lasso <- sum(fit.lassobic$coef!=0)


table_n <- matrix(data=list(len_bayes,round(r2.bms,4),len_lasso,round(r2.lassobic,4)),ncol=2)
colnames(table_n) <- c('BMA','LASSO-BIC')
rownames(table_n) <- c('Nº of Selected Variables','CV-R2')
table_n
```


```{r}
bayes_sel <- names(ci.bayesreg[,1][ci.bayesreg[,1] != 0])
lasso_sel <- names(fit.lassobic$coef[fit.lassobic$coef != 0])[-1]
lasso_dif <- lasso_sel[!lasso_sel %in% bayes_sel]
bayes_dif <- bayes_sel[!bayes_sel %in% lasso_sel]

mat <- cbind(ci.bayesreg,NA)
colnames(mat) <- c('estimate','2.5%','97.5%','margpp','color')
mat[bayes_sel,'color'] <- 4
mat[mat[,1] == 0,'color'] <- 8
mat[lasso_dif,'color'] <- 2

lasso_mat <- cbind(fit.lassobic$coef,NA)
colnames(lasso_mat) <- c('estimate','color')
lasso_mat[lasso_sel,'color'] <- 2
lasso_mat[lasso_mat[,'estimate'] == 0,'color'] <- 8
lasso_mat[bayes_dif,'color'] <- 4
```

```{r}
plot(NA, ylim=c(-6,6), xlim=c(0,nrow(mat)), ylab='Coefficient Estimate', xlab='Coefficient Index', main='Bayesian Model Averaging')
cols= mat[,'color']
points(1:nrow(mat), mat[, 1], pch = 16,col=cols)

plot(NA, ylim=c(-6,6), xlim=c(0,nrow(lasso_mat)),ylab='Coefficient Estimate', xlab='Coefficient Index', main='LASSO-BIC')
cols= lasso_mat[,'color']
points(1:nrow(lasso_mat), lasso_mat[,'estimate'], pch = 16,col=cols)
```

#### Estimated Coefficients for BMA

```{r}
ci.bayesreg[order(abs(ci.bayesreg[,'estimate']),decreasing = T),][bayes_sel,]
```

# PART 2: Unsupervised Learning

```{r}
library(readr)
library(tidyverse)
library(quanteda) # quantitative analysis of textual data  (https://quanteda.io/articles/quickstart.html)
library(quanteda.textplots) # complementary to quanteda, for visualization
library(cld3) # for language detection
library(lda) # implementation of Latent Dirichlet Allocation
library(servr) # will be used for visualization
library(stm) # for structural topic modeling
library(topicmodels)
library(LDAvis)
library(knitr)
library(wordcloud)
library(RColorBrewer)
library(sjmisc)
```

## Import Lyrics Data

```{r}
lyrics <- read.csv2(file.path(PATH,'/Proyecto Stats Data/Sergio/lyrics_concat.csv'), sep=',',header = TRUE)
lyrics
```

## Pre-processing

```{r}
lyricsCorpus <- corpus(lyrics, text_field = 'Lyrics')
```

```{r}
languages <- detect_language(lyricsCorpus)
table(languages)
```

```{r}
lyricsCorpus <- subset(lyricsCorpus, languages == "en")
```

```{r}
ntokens_corpus <- ntoken(lyricsCorpus)
data.frame(ntokens_corpus) %>% ggplot(aes(ntokens_corpus)) + geom_histogram(binwidth=10) + xlab('Number of tokens')

lyrics_length20_600 <- names(ntokens_corpus[(ntokens_corpus>=20) & (ntokens_corpus<=600)])
lyricsCorpus_filtered <- lyricsCorpus[names(lyricsCorpus) %in% lyrics_length20_600]

ntokens_corpus <- ntoken(lyricsCorpus_filtered)
data.frame(ntokens_corpus) %>% ggplot(aes(ntokens_corpus)) + geom_histogram(binwidth=10) + xlab('Number of tokens')
```

```{r}
custom_list_stopwords <- c(stopwords("en"),'url','copy','embed','urlcopyembedcopy','know','yeah','oh','ooh','verse','dit',
                           'la','chorus','thunder','s','m','embedshare','n','now','go','get','just','gonna',"ain't",'can','got',
                           'ah','one','see','pre','ma','untz','like','na','hey','want','wanna','boom','em','baby','let','give','cause',
                           'come','take','say','said','every','eh','aaahhh','deeya','badeeya','ahh','doo','al','nas','aah','eheu',
                           'bum','ohohohoh','dum','yoy','animalsmals','aghosts','ding')

dfm_lyricsCorpus<- tokens(lyricsCorpus_filtered, remove_punct=TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% 
  tokens_remove(custom_list_stopwords)  %>% dfm() %>% dfm_tolower() %>% 
  dfm_trim(min_termfreq = 2,docfreq_type = "prop")

dfm_lyricsCorpus
```


```{r,fig.height=10}
textplot_wordcloud(dfm_lyricsCorpus, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"),max_words =200,max_size = 4)
```

## Structured Topic Modelling

```{r}
fit_date <- stm(dfm_lyricsCorpus,prevalence = ~release_date, seed=123, max.em.its = 50, K=4)
```

### Topic Results

```{r fig.height=3}
plot(fit_date)
```


```{r}
labelTopics(fit_date)
```

```{r}
topics_df = data.frame('Topics'=c('Life','Journey','Nostalgia','Love'),'Text 1' = rep('',4),'Text 2' = rep('',4),'Text 3' = rep('',4))
rownames(topics_df) <- 1:4

for (topic in 1:4){
  quotes <- lyricsCorpus_filtered[order(fit_date$theta[,topic],decreasing = T)[1:3]]
  topics_df[topic,c('Text.1','Text.2','Text.3')] <- convert(quotes,to='data.frame')[,'text']
  #plotQuote(quotes, width = 100, text.cex = 1 ,main = paste("Topic ",topic))
}

topics_df
```

## Estimate Effect

```{r}
estm_eff_date <- estimateEffect(1:4 ~ release_date, fit_date, meta = docvars(lyricsCorpus_filtered))

summary(estm_eff_date)
```

```{r fig.height=3}
plot(estm_eff_date,'release_date',method='continuous',verbose.labels = F,ci.level=F)
```
